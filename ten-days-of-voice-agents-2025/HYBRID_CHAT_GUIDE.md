# ğŸ’¬ Hybrid Text/Voice Chat - Implementation Guide

## ğŸ¯ Overview

The Pregnancy Companion AI now supports **HYBRID MODE** where users can:
- ğŸ¤ **Speak** (voice input â†’ voice + text response)
- âŒ¨ï¸ **Type** (text input â†’ text-only response)
- ğŸ”„ **Switch freely** between voice and text

## ğŸ—ï¸ Architecture

### Input Flow

```
User Input
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Voice     â”‚    Text     â”‚
â”‚   Input     â”‚    Input    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“               â†“
Deepgram STT    Direct Text
    â†“               â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    Google Gemini LLM
            â†“
    Function Tools Execute
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Response    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
    â†“               â†“
Voice Mode      Text Mode
(Murf TTS)      (Text Only)
    â†“               â†“
User hears      User reads
+ sees text     text only
```

### Response Logic

```python
if input_type == "voice":
    # Voice input detected
    response_mode = "voice + text"
    # â†’ STT â†’ LLM â†’ TTS + Text
    
elif input_type == "text":
    # Text input detected
    response_mode = "text only"
    # â†’ LLM â†’ Text (NO TTS)
```

## ğŸ”§ Backend Implementation

### 1. Agent State Tracking

```python
class PregnancyCompanion(Agent):
    def __init__(self):
        # Track current input mode
        self.current_input_mode = "voice"  # or "text"
```

### 2. Text Message Handler

```python
async def handle_text_message(self, message: str) -> str:
    """
    Handle text input from user.
    Bypasses TTS, returns text-only response.
    """
    logger.info(f"ğŸ’¬ Processing text message: {message}")
    
    # Set input mode to text
    self.current_input_mode = "text"
    
    # Process through same LLM pipeline
    # Function tools work identically
    # Response is text-only (no TTS)
    
    return response_text
```

### 3. Data Packet Listener

```python
@ctx.room.on("data_received")
def on_data_received(data: rtc.DataPacket):
    """Listen for text messages from frontend."""
    message = data.data.decode('utf-8')
    
    if message.startswith("TEXT_CHAT:"):
        text_message = message.replace("TEXT_CHAT:", "").strip()
        
        # Set mode to text
        agent.current_input_mode = "text"
        
        # Process message
        # LLM handles it like voice input
```

### 4. Unified Intent Router

All inputs (voice or text) go through the same pipeline:

```python
# Voice Input
User speaks â†’ STT â†’ "I'm feeling nauseous"
    â†“
LLM detects intent â†’ analyze_symptom("nauseous")
    â†“
Response â†’ TTS + Text

# Text Input
User types â†’ "I'm feeling nauseous"
    â†“
LLM detects intent â†’ analyze_symptom("nauseous")
    â†“
Response â†’ Text Only (NO TTS)
```

## ğŸ“± Frontend Implementation

### 1. Chat Input Component

```tsx
// Add text input field
<input
  type="text"
  placeholder="Type a message or use voice..."
  onKeyPress={handleTextInput}
/>
```

### 2. Send Text Message

```typescript
function sendTextMessage(message: string) {
  // Send as data packet
  room.localParticipant.publishData(
    new TextEncoder().encode(`TEXT_CHAT:${message}`),
    { reliable: true }
  );
  
  // Add to chat UI immediately
  addMessageToChat({
    role: "user",
    content: message,
    type: "text"
  });
}
```

### 3. Receive Text Response

```typescript
room.on("data_received", (data) => {
  const message = new TextDecoder().decode(data);
  
  if (message.startsWith("AGENT_RESPONSE:")) {
    const response = message.replace("AGENT_RESPONSE:", "");
    
    // Add to chat UI
    addMessageToChat({
      role: "agent",
      content: response,
      type: "text"
    });
  }
});
```

### 4. Unified Chat History

```typescript
const chatMessages = [
  { role: "user", content: "I'm feeling nauseous", type: "voice" },
  { role: "agent", content: "Very common in first trimester...", type: "voice" },
  { role: "user", content: "Can I eat sushi?", type: "text" },
  { role: "agent", content: "âš ï¸ Raw fish should be avoided...", type: "text" },
];
```

## ğŸ¨ UI Design

### Chat Interface

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pregnancy Companion AI             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  ğŸ‘¤ I'm feeling nauseous (ğŸ¤)       â”‚
â”‚                                     â”‚
â”‚  ğŸ¤° Very common in first            â”‚
â”‚     trimester. Try ginger tea...    â”‚
â”‚                                     â”‚
â”‚  ğŸ‘¤ Can I eat sushi? (âŒ¨ï¸)           â”‚
â”‚                                     â”‚
â”‚  ğŸ¤° âš ï¸ Raw fish should be           â”‚
â”‚     avoided during pregnancy.       â”‚
â”‚                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Type message...] [ğŸ¤] [Send]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Input Modes

**Voice Mode (Default):**
- ğŸ¤ Microphone button active
- User speaks
- Agent responds with voice + text

**Text Mode:**
- âŒ¨ï¸ Text input field active
- User types
- Agent responds with text only

**Hybrid Mode:**
- Both available simultaneously
- User can switch freely
- Chat history unified

## ğŸ”„ Message Flow Examples

### Example 1: Voice â†’ Text

```
User: ğŸ¤ "How are you feeling?"
Agent: ğŸ”Š + ğŸ’¬ "I'm feeling nauseous"

User: âŒ¨ï¸ "Can I eat sushi?"
Agent: ğŸ’¬ "âš ï¸ Raw fish should be avoided during pregnancy."
```

### Example 2: Text â†’ Voice

```
User: âŒ¨ï¸ "I'm feeling anxious"
Agent: ğŸ’¬ "I hear you. Pregnancy can bring up emotions..."

User: ğŸ¤ "What should I eat?"
Agent: ğŸ”Š + ğŸ’¬ "Here are some great options: ginger tea, bananas..."
```

### Example 3: Mixed Conversation

```
User: ğŸ¤ "Hi, I need a check-in"
Agent: ğŸ”Š + ğŸ’¬ "Welcome! How are you feeling today?"

User: âŒ¨ï¸ "Tired and nauseous"
Agent: ğŸ’¬ "Fatigue is common. Let me analyze..."
       â†’ Calls analyze_symptom("nauseous")
       â†’ Calls record_fatigue_level("tired")

User: ğŸ¤ "What about nutrition?"
Agent: ğŸ”Š + ğŸ’¬ "Great question! Try ginger tea, crackers..."
```

## ğŸ› ï¸ Implementation Checklist

### Backend (Python)
- [x] Add `current_input_mode` tracking
- [x] Implement `handle_text_message()` method
- [x] Add data packet listener
- [x] Unified intent router (LLM handles both)
- [x] Response mode logic (TTS vs text-only)

### Frontend (React/TypeScript)
- [ ] Add text input component
- [ ] Implement `sendTextMessage()` function
- [ ] Add data packet listener
- [ ] Unified chat history display
- [ ] Mode indicator (ğŸ¤ vs âŒ¨ï¸)
- [ ] Message type badges

### Testing
- [ ] Voice input â†’ voice + text response
- [ ] Text input â†’ text-only response
- [ ] Switch between modes mid-conversation
- [ ] Function tools work with both inputs
- [ ] Chat history shows all messages
- [ ] Emergency detection works with text
- [ ] Nutrition checks work with text
- [ ] Task creation works with text

## ğŸ“Š Data Format

### Text Message Protocol

**Frontend â†’ Backend:**
```
TEXT_CHAT:Can I eat sushi?
```

**Backend â†’ Frontend:**
```
AGENT_RESPONSE:âš ï¸ Raw fish should be avoided during pregnancy.
```

### Chat Message Object

```typescript
interface ChatMessage {
  role: "user" | "agent";
  content: string;
  type: "voice" | "text";
  timestamp: string;
  metadata?: {
    function_called?: string;
    emergency?: boolean;
  };
}
```

## ğŸ¯ Function Tool Compatibility

All function tools work identically with voice or text input:

| Function Tool | Voice Input | Text Input |
|--------------|-------------|------------|
| `analyze_symptom()` | âœ… Works | âœ… Works |
| `record_emotional_state()` | âœ… Works | âœ… Works |
| `record_fatigue_level()` | âœ… Works | âœ… Works |
| `check_nutrition()` | âœ… Works | âœ… Works |
| `record_pregnancy_tasks()` | âœ… Works | âœ… Works |
| `save_pregnancy_journal()` | âœ… Works | âœ… Works |
| `get_weekly_pregnancy_report()` | âœ… Works | âœ… Works |
| `create_pregnancy_reminders()` | âœ… Works | âœ… Works |
| `save_to_notion()` | âœ… Works | âœ… Works |

## ğŸ” Safety Features

### Emergency Detection

Works with both voice and text:

**Voice:**
```
User: ğŸ¤ "I'm having severe bleeding"
Agent: ğŸ”Š + ğŸ’¬ "âš ï¸ This sounds like something you should discuss..."
```

**Text:**
```
User: âŒ¨ï¸ "I'm having severe bleeding"
Agent: ğŸ’¬ "âš ï¸ This sounds like something you should discuss..."
```

### Allergy Detection

Works with both inputs:

**Voice:**
```
User: ğŸ¤ "Can I eat peanuts?"
Agent: ğŸ”Š + ğŸ’¬ "âš ï¸ Peanuts contain allergen you're allergic to."
```

**Text:**
```
User: âŒ¨ï¸ "Can I eat peanuts?"
Agent: ğŸ’¬ "âš ï¸ Peanuts contain allergen you're allergic to."
```

## ğŸ“± Mobile Optimization

### Touch-Friendly Controls

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [ğŸ¤ Hold to Speak]                 â”‚
â”‚                                     â”‚
â”‚  OR                                 â”‚
â”‚                                     â”‚
â”‚  [Type message here...] [Send]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Responsive Design

- **Mobile:** Stack voice/text inputs vertically
- **Desktop:** Side-by-side inputs
- **Tablet:** Adaptive layout

## ğŸ¨ UI Components Needed

### 1. Chat Message Component

```tsx
<ChatMessage
  role="user"
  content="I'm feeling nauseous"
  type="voice"
  timestamp="2025-11-28T12:00:00"
/>
```

### 2. Input Mode Selector

```tsx
<InputModeSelector
  mode={inputMode}
  onModeChange={setInputMode}
/>
```

### 3. Text Input Field

```tsx
<TextInput
  placeholder="Type a message..."
  onSend={sendTextMessage}
  disabled={isVoiceActive}
/>
```

### 4. Voice Button

```tsx
<VoiceButton
  isActive={isVoiceActive}
  onPress={startVoice}
  onRelease={stopVoice}
/>
```

## ğŸš€ Next Steps

### Phase 1: Backend (Complete)
- [x] Add input mode tracking
- [x] Implement text message handler
- [x] Add data packet listener
- [x] Document architecture

### Phase 2: Frontend (To Do)
- [ ] Create chat UI components
- [ ] Add text input field
- [ ] Implement message sending
- [ ] Add message receiving
- [ ] Style chat interface

### Phase 3: Testing (To Do)
- [ ] Test voice input
- [ ] Test text input
- [ ] Test mode switching
- [ ] Test function tools
- [ ] Test emergency detection

### Phase 4: Polish (To Do)
- [ ] Add typing indicators
- [ ] Add message timestamps
- [ ] Add message status (sent/delivered)
- [ ] Add scroll to bottom
- [ ] Add message search

## ğŸ’¡ Pro Tips

1. **Default to Voice** - Voice is primary, text is supplementary
2. **Clear Indicators** - Show which mode is active
3. **Unified History** - All messages in one timeline
4. **Smooth Transitions** - No jarring switches between modes
5. **Mobile First** - Optimize for touch interactions

## ğŸ†˜ Troubleshooting

**Text messages not sending:**
- Check data packet encoding
- Verify "TEXT_CHAT:" prefix
- Check network connection

**Agent not responding to text:**
- Verify `current_input_mode` is set
- Check LLM is processing text
- Verify function tools are callable

**TTS playing for text input:**
- Check input mode detection
- Verify TTS bypass logic
- Check response routing

## ğŸ“š References

- LiveKit Data Packets: https://docs.livekit.io/realtime/client/data-messages/
- Agent Session: https://docs.livekit.io/agents/build/agent-session/
- Chat Context: https://docs.livekit.io/agents/build/chat-context/

---

**ğŸ‰ Hybrid text/voice mode is ready! Users can now speak OR type to their Pregnancy Companion AI!** ğŸ’¬ğŸ¤
